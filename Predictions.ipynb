{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the usual suspects\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixels : (5000, 3072) Overfeat: (5000, 4096) Labels: (5000,) Names: (4,) Allow pickle: ()\n"
     ]
    }
   ],
   "source": [
    "# Load the train data\n",
    "\n",
    "with np.load('/Users/David/Desktop/EPFL Applied ML/cifar4-train.npz', allow_pickle=False) as npz_file:\n",
    "    pixels = npz_file['pixels'].astype('float32')\n",
    "    overfeat = npz_file['overfeat']\n",
    "    labels = npz_file['labels']\n",
    "    names = npz_file['names']\n",
    "    allow = npz_file['allow_pickle']\n",
    "    \n",
    "print('Pixels : {:}'.format(pixels.shape),\n",
    "      'Overfeat: {:}'.format(overfeat.shape),\n",
    "      'Labels: {:}'.format(labels.shape),\n",
    "      'Names: {:}'.format(names.shape), \n",
    "      'Allow pickle: {:}'.format(allow.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixels : (5000, 3072) Overfeat: (5000, 4096) Allow pickle: ()\n"
     ]
    }
   ],
   "source": [
    "# Load the test data\n",
    "\n",
    "with np.load('/Users/David/Desktop/EPFL Applied ML/cifar4-test.npz', allow_pickle=False) as npz_file:\n",
    "    pixel_te = npz_file['pixels'].astype('float32')\n",
    "    overfeat_te = npz_file['overfeat']\n",
    "    allow_te = npz_file['allow_pickle']\n",
    "    \n",
    "print('Pixels : {:}'.format(pixels.shape),\n",
    "      'Overfeat: {:}'.format(overfeat.shape),\n",
    "      'Allow pickle: {:}'.format(allow.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the train data by scaling to improve performance of the gradient descent optimizer\n",
    "\n",
    "scaler = StandardScaler().fit(overfeat)\n",
    "X_train_standard = scaler.transform(overfeat)\n",
    "X_test_standard = scaler.transform(overfeat_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will be used to select batches of data, using a Python generator\n",
    "\n",
    "def get_batches(X, y, batch_size):\n",
    "    # Shuffle X,y\n",
    "    shuffled_idx = np.arange(len(y)) # 1,2,...,n\n",
    "    np.random.shuffle(shuffled_idx)\n",
    "\n",
    "    # Enumerate indexes by steps of batch_size\n",
    "    # i: 0, b, 2b, 3b, 4b, .. where b is the batch size\n",
    "    for i in range(0, len(y), batch_size):\n",
    "        # Batch indexes\n",
    "        batch_idx = shuffled_idx[i:i+batch_size]\n",
    "        yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Create placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 4096]) # dimensions set to 4096, as they correspond to the overfeat shape\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    dropout = tf.placeholder(dtype=tf.bool) # placeholder to pass to layers.dropout in order to deactivate some neurons\n",
    "    alpha = tf.placeholder(dtype=tf.float32) # placeholder to pass as regularization term\n",
    "    \n",
    "    # Hidden layer with 64 units\n",
    "    hidden = tf.layers.dense(\n",
    "        X, 64, activation=tf.nn.relu, # ReLU activation\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0), # initializer for layers with ReLU activation\n",
    "        bias_initializer=tf.zeros_initializer(), # all zeros initializer\n",
    "        name='hidden'\n",
    "    )\n",
    "\n",
    "    # Apply dropout to hidden layer\n",
    "    hidden = tf.layers.dropout(\n",
    "        hidden, rate=0.5, seed=0, training=dropout)\n",
    "    \n",
    "    # Get weights/biases of the hidden layer\n",
    "    with tf.variable_scope('hidden', reuse=True):\n",
    "        W1 = tf.get_variable('kernel')\n",
    "        b1 = tf.get_variable('bias')    \n",
    "    \n",
    "    # Output layer with 4 logits\n",
    "    logits = tf.layers.dense(\n",
    "        hidden, 4, activation=None, # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=0), # initializer for layers without ReLU activation\n",
    "        bias_initializer=tf.zeros_initializer(), # all zeros initializer\n",
    "        name='output'\n",
    "    )\n",
    "\n",
    "    # Get weights/biases of the output layer\n",
    "    with tf.variable_scope('output', reuse=True):\n",
    "        W2 = tf.get_variable('kernel')\n",
    "        b2 = tf.get_variable('bias')\n",
    "    \n",
    "    # Loss fuction: mean cross-entropy with regularization term\n",
    "    mean_ce = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=y, logits=logits)) # mean cross-entropy\n",
    "    l2_term = tf.nn.l2_loss(W1) + tf.nn.l2_loss(W1) # L2 term which includes the hidden and output weight matrices\n",
    "    loss = mean_ce + alpha * l2_term # total loss with penalization\n",
    "    \n",
    "    # Gradient descent parameters\n",
    "    lr = tf.placeholder(dtype=tf.float32) # learning rate\n",
    "    gd = tf.train.GradientDescentOptimizer(learning_rate=lr) # gradient descent algorithm\n",
    "\n",
    "    # Minimize the loss function (cross-entropy with L2 regularization)\n",
    "    train_op = gd.minimize(loss)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32) # Class with maximum logit\n",
    "    is_correct = tf.equal(y, predictions) # Compare predictions to target values\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32)) # mean value of correctly predicted logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - train: 0.662 (mean)\n",
      "Epoch 2 - train: 0.775 (mean)\n",
      "Epoch 3 - train: 0.801 (mean)\n",
      "Epoch 4 - train: 0.819 (mean)\n",
      "Epoch 5 - train: 0.827 (mean)\n",
      "Epoch 6 - train: 0.829 (mean)\n",
      "Epoch 7 - train: 0.838 (mean)\n",
      "Epoch 8 - train: 0.849 (mean)\n",
      "Epoch 9 - train: 0.854 (mean)\n",
      "Epoch 10 - train: 0.864 (mean)\n",
      "Epoch 11 - train: 0.861 (mean)\n",
      "Epoch 12 - train: 0.867 (mean)\n",
      "Epoch 13 - train: 0.874 (mean)\n",
      "Epoch 14 - train: 0.877 (mean)\n",
      "Epoch 15 - train: 0.881 (mean)\n",
      "Epoch 16 - train: 0.880 (mean)\n",
      "Epoch 17 - train: 0.893 (mean)\n",
      "Epoch 18 - train: 0.887 (mean)\n",
      "Epoch 19 - train: 0.892 (mean)\n",
      "Epoch 20 - train: 0.889 (mean)\n",
      "Epoch 21 - train: 0.894 (mean)\n",
      "Epoch 22 - train: 0.902 (mean)\n",
      "Epoch 23 - train: 0.906 (mean)\n",
      "Epoch 24 - train: 0.912 (mean)\n",
      "Epoch 25 - train: 0.907 (mean)\n",
      "Epoch 26 - train: 0.909 (mean)\n",
      "Epoch 27 - train: 0.911 (mean)\n",
      "Epoch 28 - train: 0.918 (mean)\n",
      "Epoch 29 - train: 0.914 (mean)\n",
      "Epoch 30 - train: 0.919 (mean)\n",
      "Epoch 31 - train: 0.923 (mean)\n",
      "Epoch 32 - train: 0.929 (mean)\n",
      "Epoch 33 - train: 0.923 (mean)\n",
      "Epoch 34 - train: 0.926 (mean)\n",
      "Epoch 35 - train: 0.935 (mean)\n",
      "Epoch 36 - train: 0.934 (mean)\n",
      "Epoch 37 - train: 0.934 (mean)\n",
      "Epoch 38 - train: 0.931 (mean)\n",
      "Epoch 39 - train: 0.938 (mean)\n",
      "Epoch 40 - train: 0.935 (mean)\n"
     ]
    }
   ],
   "source": [
    "# Rerun the neural network with the tuned values found in for this fully connected model. \n",
    "\n",
    "train_acc_values = [] # empty list to fill with mean training accuracy across batches\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Set seed\n",
    "    np.random.seed(0)\n",
    "            \n",
    "    for epoch in range(40):\n",
    "        # Accuracy values (train) after each batch\n",
    "        batch_acc = []\n",
    "\n",
    "        # Get batches of data of size 64\n",
    "        for X_batch, y_batch in get_batches(X_train_standard, labels, 64):\n",
    "            # Run training and evaluate accuracy\n",
    "            _, acc_value = sess.run([train_op, accuracy], feed_dict={\n",
    "                X: X_batch,\n",
    "                y: y_batch,\n",
    "                lr: 0.01, # Learning rate\n",
    "                alpha: 0.0001, # Regularization term\n",
    "                dropout: True # Apply dropout for the training set\n",
    "            })\n",
    "\n",
    "            # Save accuracy (current batch)\n",
    "            batch_acc.append(acc_value)\n",
    "        train_acc_values.append(np.mean(batch_acc))\n",
    "\n",
    "        # Print progress for the epochs\n",
    "        print('Epoch {} - train: {:.3f} (mean)'.format(\n",
    "            epoch+1, np.mean(batch_acc)\n",
    "        ))        \n",
    " \n",
    "    # Predictions\n",
    "    class_prediction = sess.run(predictions, feed_dict={\n",
    "        X: X_test_standard,\n",
    "        dropout: False # do not apply dropout because it is the test set, and we are not fitting anything\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('predictions', class_prediction, allow_pickle=False) # save the predicted classes to a npy file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
