{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#Load the usual suspects\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixels : (5000, 3072) Overfeat: (5000, 4096) Labels: (5000,) Names: (4,) Allow pickle: ()\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "with np.load('/Users/David/Desktop/EPFL Applied ML/cifar4-train.npz', allow_pickle=False) as npz_file:\n",
    "    pixels = npz_file['pixels'].astype('float32')\n",
    "    overfeat = npz_file['overfeat']\n",
    "    labels = npz_file['labels']\n",
    "    names = npz_file['names']\n",
    "    allow = npz_file['allow_pickle']\n",
    "    \n",
    "print('Pixels : {:}'.format(pixels.shape),\n",
    "      'Overfeat: {:}'.format(overfeat.shape),\n",
    "      'Labels: {:}'.format(labels.shape),\n",
    "      'Names: {:}'.format(names.shape), \n",
    "      'Allow pickle: {:}'.format(allow.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (3200, 4096) Test set: (1000, 4096) Validation set: (800, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Rename the data and split into train (3200), test (1000) and validation (800) sets\n",
    "X_ov = overfeat\n",
    "X_px = pixels\n",
    "y = labels\n",
    "\n",
    "X_ov_tr, X_ov_te, X_px_tr, X_px_te, y_tr, y_te = train_test_split(X_ov, X_px, y, test_size=1000, stratify=y, random_state=0)\n",
    "X_ov_tr, X_ov_val, X_px_tr, X_px_val, y_tr, y_val = train_test_split(X_ov_tr, X_px_tr, y_tr, test_size=800, stratify=y_tr, random_state=0)\n",
    "\n",
    "# Double check dimensions for the overfeat data:\n",
    "print('Train set: {:}'.format(X_ov_tr.shape), 'Test set: {:}'.format(X_ov_te.shape), 'Validation set: {:}'.format(X_ov_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data by scaling to improve performance of the gradient descent optimizer\n",
    "\n",
    "scaler = StandardScaler().fit(X_ov_tr)\n",
    "X_train_standard = scaler.transform(X_ov_tr)\n",
    "X_valid_standard = scaler.transform(X_ov_val)\n",
    "X_test_standard = scaler.transform(X_ov_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will be used to select batches of data, using a Python generator\n",
    "\n",
    "def get_batches(X, y, batch_size):\n",
    "    # Shuffle X,y\n",
    "    shuffled_idx = np.arange(len(y)) # 1,2,...,n\n",
    "    np.random.shuffle(shuffled_idx)\n",
    "\n",
    "    # Enumerate indexes by steps of batch_size\n",
    "    # i: 0, b, 2b, 3b, 4b, .. where b is the batch size\n",
    "    for i in range(0, len(y), batch_size):\n",
    "        # Batch indexes\n",
    "        batch_idx = shuffled_idx[i:i+batch_size]\n",
    "        yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Create placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 4096]) # dimensions set to 4096, as they correspond to the overfeat shape\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    dropout = tf.placeholder(dtype=tf.bool) # placeholder to pass to layers.dropout in order to deactivate some neurons\n",
    "    alpha = tf.placeholder(dtype=tf.float32) # placeholder to pass as regularization term\n",
    "    \n",
    "    # Hidden layer with 64 units\n",
    "    hidden = tf.layers.dense(\n",
    "        X, 64, activation=tf.nn.relu, # ReLU activation\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0), # initializer for layers with ReLU activation\n",
    "        bias_initializer=tf.zeros_initializer(), # all zeros initializer\n",
    "        name='hidden'\n",
    "    )\n",
    "\n",
    "    # Apply dropout to hidden layer\n",
    "    hidden = tf.layers.dropout(\n",
    "        hidden, rate=0.5, seed=0, training=dropout)\n",
    "    \n",
    "    # Get weights/biases of the hidden layer\n",
    "    with tf.variable_scope('hidden', reuse=True):\n",
    "        W1 = tf.get_variable('kernel')\n",
    "        b1 = tf.get_variable('bias')    \n",
    "    \n",
    "    # Output layer with 4 logits\n",
    "    logits = tf.layers.dense(\n",
    "        hidden, 4, activation=None, # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=0), # initializer for layers without ReLU activation\n",
    "        bias_initializer=tf.zeros_initializer(), # all zeros initializer\n",
    "        name='output'\n",
    "    )\n",
    "\n",
    "    # Get weights/biases of the output layer\n",
    "    with tf.variable_scope('output', reuse=True):\n",
    "        W2 = tf.get_variable('kernel')\n",
    "        b2 = tf.get_variable('bias')\n",
    "    \n",
    "    # Loss fuction: mean cross-entropy with regularization term\n",
    "    mean_ce = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=y, logits=logits)) # mean cross-entropy\n",
    "    l2_term = tf.nn.l2_loss(W1) + tf.nn.l2_loss(W1) # L2 term which includes the hidden and output weight matrices\n",
    "    loss = mean_ce + alpha * l2_term # total loss with penalization\n",
    "    \n",
    "    # Gradient descent parameters\n",
    "    lr = tf.placeholder(dtype=tf.float32) # learning rate\n",
    "    gd = tf.train.GradientDescentOptimizer(learning_rate=lr) # gradient descent algorithm\n",
    "\n",
    "    # Minimize the loss function (cross-entropy with L2 regularization)\n",
    "    train_op = gd.minimize(loss)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32) # Class with maximum logit\n",
    "    is_correct = tf.equal(y, predictions) # Compare predictions to target values\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32)) # mean value of correctly predicted logits\n",
    "    \n",
    "train_acc_values = [] # empty list to fill with mean training accuracy across batches\n",
    "valid_acc_values = [] # empty list to fill with validation accuracy\n",
    "\n",
    "mean_train = [] # empty list to fill with the mean training accuracy for the last 3 epochs\n",
    "mean_valid = [] # empty list to fill with the mean validation accuracy for the last 3 epochs\n",
    "epoch_total = [] # empty list to fill with the epoch values\n",
    "alpha_total = [] # empty list to fill with the regularization strength values\n",
    "learning_total = [] # empty list to fill with the regularization strength values\n",
    "\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001] #list of learning rates to iterate from. Tested >=1 and always yields 0.25 for both training and validation test\n",
    "alphas = [0.1, 0.01, 0.001, 0.0001] # list of regularization strengths to iterate from\n",
    "epochs = [10, 15, 20, 30] # list of epochs to iterate from\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    \n",
    "    # Set seed\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Start nested loops to iterate through the epoch, alphas, and learning rate:\n",
    "    for learning_value in learning_rates:\n",
    "        for alpha_value in alphas: \n",
    "            for i in np.arange(len(epochs)): \n",
    "                epoch_total.append(epochs[i]) # number of epochs\n",
    "                alpha_total.append(alpha_value) # regularization value\n",
    "                learning_total.append(learning_value) # learning rates\n",
    "            \n",
    "                # Initialize variables - do it inside the parameter loops and before the epochs otherwise it will carry on with the learning from the previous steps\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "                for epoch in range(epochs[i]):\n",
    "                    # Accuracy values (train) after each batch\n",
    "                    batch_acc = []\n",
    "\n",
    "                    # Get batches of data of size 64\n",
    "                    for X_batch, y_batch in get_batches(X_train_standard, y_tr, 64):\n",
    "                        # Run training and evaluate accuracy\n",
    "                        _, acc_value = sess.run([train_op, accuracy], feed_dict={\n",
    "                            X: X_batch,\n",
    "                            y: y_batch,\n",
    "                            lr: learning_value, # Learning rate\n",
    "                            alpha: alpha_value, # Regularization term\n",
    "                            dropout: True # Apply dropout for the training set\n",
    "                        })\n",
    "\n",
    "                        # Save accuracy (current batch)\n",
    "                        batch_acc.append(acc_value)\n",
    "                    train_acc_values.append(np.mean(batch_acc))\n",
    "            \n",
    "                    # Evaluate validation accuracy\n",
    "                    valid_acc = sess.run(accuracy, feed_dict={\n",
    "                        X: X_valid_standard,\n",
    "                        y: y_val,\n",
    "                        dropout: False # do not apply dropout because it is the validation set, and we are not fitting anything\n",
    "                    })\n",
    "                    valid_acc_values.append(valid_acc)\n",
    "\n",
    "                # Print progress for the latest value of the epoch - uncheck to allow printing of results\n",
    "                #print('Epoch {} - valid: {:.3f} train: {:.3f} (mean)'.format(\n",
    "                #    epoch+1, valid_acc, np.mean(batch_acc)\n",
    "                #))   \n",
    "                \n",
    "                mean_train.append(np.mean(train_acc_values[-3:])) # save mean values of the 3 last epochs\n",
    "                mean_valid.append(np.mean(valid_acc_values[-3:]))     \n",
    "        \n",
    "    summary = pd.DataFrame({'mean train accuracy': mean_train, # put everything nicely into a dataframe\n",
    "                            'mean validation accuracy': mean_valid, \n",
    "                            'epochs': epoch_total, \n",
    "                            'alpha': alpha_total, \n",
    "                            'learning rate': learning_total})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>epochs</th>\n",
       "      <th>learning rate</th>\n",
       "      <th>mean train accuracy</th>\n",
       "      <th>mean validation accuracy</th>\n",
       "      <th>delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.921771</td>\n",
       "      <td>0.848333</td>\n",
       "      <td>0.073438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.925729</td>\n",
       "      <td>0.844167</td>\n",
       "      <td>0.081562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.899688</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.056354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.895938</td>\n",
       "      <td>0.842083</td>\n",
       "      <td>0.053854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.872188</td>\n",
       "      <td>0.840833</td>\n",
       "      <td>0.031354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.882188</td>\n",
       "      <td>0.839583</td>\n",
       "      <td>0.042604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.927813</td>\n",
       "      <td>0.839167</td>\n",
       "      <td>0.088646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.878750</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.041250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.892917</td>\n",
       "      <td>0.836667</td>\n",
       "      <td>0.056250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.854479</td>\n",
       "      <td>0.836667</td>\n",
       "      <td>0.017813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>0.836250</td>\n",
       "      <td>0.062187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>15</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.897813</td>\n",
       "      <td>0.836250</td>\n",
       "      <td>0.061563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>30</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.928437</td>\n",
       "      <td>0.835417</td>\n",
       "      <td>0.093021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.852917</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.017917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.851458</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.016458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.870938</td>\n",
       "      <td>0.834583</td>\n",
       "      <td>0.036354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>30</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.865937</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.032604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.879167</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.045833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>15</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.885208</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.052708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>30</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.936875</td>\n",
       "      <td>0.831667</td>\n",
       "      <td>0.105208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.852500</td>\n",
       "      <td>0.831667</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.873854</td>\n",
       "      <td>0.831250</td>\n",
       "      <td>0.042604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>20</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.908542</td>\n",
       "      <td>0.831250</td>\n",
       "      <td>0.077292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>20</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.910833</td>\n",
       "      <td>0.830417</td>\n",
       "      <td>0.080417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.815417</td>\n",
       "      <td>0.824167</td>\n",
       "      <td>-0.008750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.808438</td>\n",
       "      <td>0.823750</td>\n",
       "      <td>-0.015312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.804479</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>-0.018438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.846042</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>0.023125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>15</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.862708</td>\n",
       "      <td>0.822500</td>\n",
       "      <td>0.040208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>20</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.865313</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.045313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.805937</td>\n",
       "      <td>0.804583</td>\n",
       "      <td>0.001354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.763333</td>\n",
       "      <td>0.804583</td>\n",
       "      <td>-0.041250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.788646</td>\n",
       "      <td>0.804167</td>\n",
       "      <td>-0.015521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.737083</td>\n",
       "      <td>0.801250</td>\n",
       "      <td>-0.064167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.766562</td>\n",
       "      <td>0.799167</td>\n",
       "      <td>-0.032604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.769792</td>\n",
       "      <td>0.795833</td>\n",
       "      <td>-0.026042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.774896</td>\n",
       "      <td>0.794583</td>\n",
       "      <td>-0.019687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>15</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.763125</td>\n",
       "      <td>0.792083</td>\n",
       "      <td>-0.028958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.769063</td>\n",
       "      <td>0.790417</td>\n",
       "      <td>-0.021354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.769063</td>\n",
       "      <td>0.788750</td>\n",
       "      <td>-0.019687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.735521</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>-0.045729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.769479</td>\n",
       "      <td>0.780417</td>\n",
       "      <td>-0.010937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.737813</td>\n",
       "      <td>0.780417</td>\n",
       "      <td>-0.042604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.728229</td>\n",
       "      <td>0.771667</td>\n",
       "      <td>-0.043437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.596562</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>-0.108438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.610625</td>\n",
       "      <td>0.703333</td>\n",
       "      <td>-0.092708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.697500</td>\n",
       "      <td>-0.098750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.598021</td>\n",
       "      <td>0.692083</td>\n",
       "      <td>-0.094063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.549375</td>\n",
       "      <td>0.660833</td>\n",
       "      <td>-0.111458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.549062</td>\n",
       "      <td>0.657917</td>\n",
       "      <td>-0.108854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.534583</td>\n",
       "      <td>0.647083</td>\n",
       "      <td>-0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.518125</td>\n",
       "      <td>0.635417</td>\n",
       "      <td>-0.117292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.482188</td>\n",
       "      <td>0.595417</td>\n",
       "      <td>-0.113229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.474271</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>-0.115729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.513437</td>\n",
       "      <td>0.580417</td>\n",
       "      <td>-0.066979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.471875</td>\n",
       "      <td>0.557917</td>\n",
       "      <td>-0.086042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.440313</td>\n",
       "      <td>0.525833</td>\n",
       "      <td>-0.085521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.422604</td>\n",
       "      <td>0.495417</td>\n",
       "      <td>-0.072812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.399479</td>\n",
       "      <td>0.494583</td>\n",
       "      <td>-0.095104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.393750</td>\n",
       "      <td>0.403750</td>\n",
       "      <td>-0.010000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alpha  epochs  learning rate  mean train accuracy  \\\n",
       "31  0.0001      30         0.0100             0.921771   \n",
       "23  0.0100      30         0.0100             0.925729   \n",
       "30  0.0001      20         0.0100             0.899688   \n",
       "26  0.0010      20         0.0100             0.895938   \n",
       "17  0.1000      15         0.0100             0.872188   \n",
       "18  0.1000      20         0.0100             0.882188   \n",
       "27  0.0010      30         0.0100             0.927813   \n",
       "21  0.0100      15         0.0100             0.878750   \n",
       "19  0.1000      30         0.0100             0.892917   \n",
       "20  0.0100      10         0.0100             0.854479   \n",
       "22  0.0100      20         0.0100             0.898438   \n",
       "13  0.0001      15         0.1000             0.897813   \n",
       "11  0.0010      30         0.1000             0.928437   \n",
       "24  0.0010      10         0.0100             0.852917   \n",
       "28  0.0001      10         0.0100             0.851458   \n",
       "12  0.0001      10         0.1000             0.870938   \n",
       "7   0.0100      30         0.1000             0.865937   \n",
       "25  0.0010      15         0.0100             0.879167   \n",
       "9   0.0010      15         0.1000             0.885208   \n",
       "15  0.0001      30         0.1000             0.936875   \n",
       "16  0.1000      10         0.0100             0.852500   \n",
       "29  0.0001      15         0.0100             0.873854   \n",
       "10  0.0010      20         0.1000             0.908542   \n",
       "14  0.0001      20         0.1000             0.910833   \n",
       "35  0.1000      30         0.0010             0.815417   \n",
       "39  0.0100      30         0.0010             0.808438   \n",
       "43  0.0010      30         0.0010             0.804479   \n",
       "4   0.0100      10         0.1000             0.846042   \n",
       "5   0.0100      15         0.1000             0.862708   \n",
       "6   0.0100      20         0.1000             0.865313   \n",
       "..     ...     ...            ...                  ...   \n",
       "47  0.0001      30         0.0010             0.805937   \n",
       "41  0.0010      15         0.0010             0.763333   \n",
       "38  0.0100      20         0.0010             0.788646   \n",
       "44  0.0001      10         0.0010             0.737083   \n",
       "45  0.0001      15         0.0010             0.766562   \n",
       "37  0.0100      15         0.0010             0.769792   \n",
       "33  0.1000      15         0.0010             0.774896   \n",
       "1   0.1000      15         0.1000             0.763125   \n",
       "3   0.1000      30         0.1000             0.769063   \n",
       "0   0.1000      10         0.1000             0.769063   \n",
       "32  0.1000      10         0.0010             0.735521   \n",
       "2   0.1000      20         0.1000             0.769479   \n",
       "36  0.0100      10         0.0010             0.737813   \n",
       "40  0.0010      10         0.0010             0.728229   \n",
       "59  0.0010      30         0.0001             0.596562   \n",
       "51  0.1000      30         0.0001             0.610625   \n",
       "55  0.0100      30         0.0001             0.598750   \n",
       "63  0.0001      30         0.0001             0.598021   \n",
       "58  0.0010      20         0.0001             0.549375   \n",
       "62  0.0001      20         0.0001             0.549062   \n",
       "54  0.0100      20         0.0001             0.534583   \n",
       "53  0.0100      15         0.0001             0.518125   \n",
       "49  0.1000      15         0.0001             0.482188   \n",
       "61  0.0001      15         0.0001             0.474271   \n",
       "50  0.1000      20         0.0001             0.513437   \n",
       "57  0.0010      15         0.0001             0.471875   \n",
       "48  0.1000      10         0.0001             0.440313   \n",
       "52  0.0100      10         0.0001             0.422604   \n",
       "56  0.0010      10         0.0001             0.399479   \n",
       "60  0.0001      10         0.0001             0.393750   \n",
       "\n",
       "    mean validation accuracy     delta  \n",
       "31                  0.848333  0.073438  \n",
       "23                  0.844167  0.081562  \n",
       "30                  0.843333  0.056354  \n",
       "26                  0.842083  0.053854  \n",
       "17                  0.840833  0.031354  \n",
       "18                  0.839583  0.042604  \n",
       "27                  0.839167  0.088646  \n",
       "21                  0.837500  0.041250  \n",
       "19                  0.836667  0.056250  \n",
       "20                  0.836667  0.017813  \n",
       "22                  0.836250  0.062187  \n",
       "13                  0.836250  0.061563  \n",
       "11                  0.835417  0.093021  \n",
       "24                  0.835000  0.017917  \n",
       "28                  0.835000  0.016458  \n",
       "12                  0.834583  0.036354  \n",
       "7                   0.833333  0.032604  \n",
       "25                  0.833333  0.045833  \n",
       "9                   0.832500  0.052708  \n",
       "15                  0.831667  0.105208  \n",
       "16                  0.831667  0.020833  \n",
       "29                  0.831250  0.042604  \n",
       "10                  0.831250  0.077292  \n",
       "14                  0.830417  0.080417  \n",
       "35                  0.824167 -0.008750  \n",
       "39                  0.823750 -0.015312  \n",
       "43                  0.822917 -0.018438  \n",
       "4                   0.822917  0.023125  \n",
       "5                   0.822500  0.040208  \n",
       "6                   0.820000  0.045313  \n",
       "..                       ...       ...  \n",
       "47                  0.804583  0.001354  \n",
       "41                  0.804583 -0.041250  \n",
       "38                  0.804167 -0.015521  \n",
       "44                  0.801250 -0.064167  \n",
       "45                  0.799167 -0.032604  \n",
       "37                  0.795833 -0.026042  \n",
       "33                  0.794583 -0.019687  \n",
       "1                   0.792083 -0.028958  \n",
       "3                   0.790417 -0.021354  \n",
       "0                   0.788750 -0.019687  \n",
       "32                  0.781250 -0.045729  \n",
       "2                   0.780417 -0.010937  \n",
       "36                  0.780417 -0.042604  \n",
       "40                  0.771667 -0.043437  \n",
       "59                  0.705000 -0.108438  \n",
       "51                  0.703333 -0.092708  \n",
       "55                  0.697500 -0.098750  \n",
       "63                  0.692083 -0.094063  \n",
       "58                  0.660833 -0.111458  \n",
       "62                  0.657917 -0.108854  \n",
       "54                  0.647083 -0.112500  \n",
       "53                  0.635417 -0.117292  \n",
       "49                  0.595417 -0.113229  \n",
       "61                  0.590000 -0.115729  \n",
       "50                  0.580417 -0.066979  \n",
       "57                  0.557917 -0.086042  \n",
       "48                  0.525833 -0.085521  \n",
       "52                  0.495417 -0.072812  \n",
       "56                  0.494583 -0.095104  \n",
       "60                  0.403750 -0.010000  \n",
       "\n",
       "[64 rows x 6 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary['delta'] = summary['mean train accuracy'] - summary['mean validation accuracy'] #define a column which is the difference between the train and validation accuracies\n",
    "summary.sort_values(by=['mean validation accuracy'], ascending=False) \n",
    "#summary\n",
    "# After analyzing the summary results, it seems that the best compromise between a high validation accuracy and a low overfitting of the training set (therefore low delta) is the NN with alpha 0.0001, 20 or 30 epochs and learning rate 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - valid: 0.776 train: 0.625 (mean)\n",
      "Epoch 2 - valid: 0.798 train: 0.757 (mean)\n",
      "Epoch 3 - valid: 0.815 train: 0.797 (mean)\n",
      "Epoch 4 - valid: 0.819 train: 0.813 (mean)\n",
      "Epoch 5 - valid: 0.816 train: 0.823 (mean)\n",
      "Epoch 6 - valid: 0.812 train: 0.833 (mean)\n",
      "Epoch 7 - valid: 0.821 train: 0.846 (mean)\n",
      "Epoch 8 - valid: 0.817 train: 0.854 (mean)\n",
      "Epoch 9 - valid: 0.825 train: 0.848 (mean)\n",
      "Epoch 10 - valid: 0.822 train: 0.854 (mean)\n",
      "Epoch 11 - valid: 0.834 train: 0.867 (mean)\n",
      "Epoch 12 - valid: 0.837 train: 0.870 (mean)\n",
      "Epoch 13 - valid: 0.831 train: 0.872 (mean)\n",
      "Epoch 14 - valid: 0.835 train: 0.877 (mean)\n",
      "Epoch 15 - valid: 0.840 train: 0.877 (mean)\n",
      "Epoch 16 - valid: 0.842 train: 0.887 (mean)\n",
      "Epoch 17 - valid: 0.841 train: 0.885 (mean)\n",
      "Epoch 18 - valid: 0.839 train: 0.898 (mean)\n",
      "Epoch 19 - valid: 0.837 train: 0.903 (mean)\n",
      "Epoch 20 - valid: 0.845 train: 0.897 (mean)\n",
      "Epoch 21 - valid: 0.839 train: 0.901 (mean)\n",
      "Epoch 22 - valid: 0.840 train: 0.903 (mean)\n",
      "Epoch 23 - valid: 0.841 train: 0.905 (mean)\n",
      "Epoch 24 - valid: 0.839 train: 0.918 (mean)\n",
      "Epoch 25 - valid: 0.844 train: 0.917 (mean)\n",
      "Epoch 26 - valid: 0.842 train: 0.919 (mean)\n",
      "Epoch 27 - valid: 0.844 train: 0.918 (mean)\n",
      "Epoch 28 - valid: 0.840 train: 0.923 (mean)\n",
      "Epoch 29 - valid: 0.840 train: 0.923 (mean)\n",
      "Epoch 30 - valid: 0.844 train: 0.931 (mean)\n",
      "Epoch 31 - valid: 0.844 train: 0.925 (mean)\n",
      "Epoch 32 - valid: 0.845 train: 0.925 (mean)\n",
      "Epoch 33 - valid: 0.841 train: 0.934 (mean)\n",
      "Epoch 34 - valid: 0.848 train: 0.937 (mean)\n",
      "Epoch 35 - valid: 0.837 train: 0.935 (mean)\n",
      "Epoch 36 - valid: 0.842 train: 0.937 (mean)\n",
      "Epoch 37 - valid: 0.846 train: 0.937 (mean)\n",
      "Epoch 38 - valid: 0.840 train: 0.947 (mean)\n",
      "Epoch 39 - valid: 0.846 train: 0.946 (mean)\n",
      "Epoch 40 - valid: 0.845 train: 0.948 (mean)\n",
      "Multilayer fully-connected network accuracy (test set): 0.832\n"
     ]
    }
   ],
   "source": [
    "# Rerun the neural network with the tuned values found in the previous step. Epochs are increased to 40 to tune even further the accuracy.\n",
    "\n",
    "train_acc_values = [] # empty list to fill with mean training accuracy across batches\n",
    "valid_acc_values = [] # empty list to fill with validation accuracy\n",
    "\n",
    "mean_train = [] # empty list to fill with the mean training accuracy for the last 3 epochs\n",
    "mean_valid = [] # empty list to fill with the mean validation accuracy for the last 3 epochs\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Set seed\n",
    "    np.random.seed(0)\n",
    "            \n",
    "    for epoch in range(40):\n",
    "        # Accuracy values (train) after each batch\n",
    "        batch_acc = []\n",
    "\n",
    "        # Get batches of data of size 64\n",
    "        for X_batch, y_batch in get_batches(X_train_standard, y_tr, 64):\n",
    "            # Run training and evaluate accuracy\n",
    "            _, acc_value = sess.run([train_op, accuracy], feed_dict={\n",
    "                X: X_batch,\n",
    "                y: y_batch,\n",
    "                lr: 0.01, # Learning rate\n",
    "                alpha: 0.0001, # Regularization term\n",
    "                dropout: True # Apply dropout for the training set\n",
    "            })\n",
    "\n",
    "            # Save accuracy (current batch)\n",
    "            batch_acc.append(acc_value)\n",
    "        train_acc_values.append(np.mean(batch_acc))\n",
    "            \n",
    "        # Evaluate validation accuracy\n",
    "        valid_acc = sess.run(accuracy, feed_dict={\n",
    "            X: X_valid_standard,\n",
    "            y: y_val,\n",
    "            dropout: False # do not apply dropout because it is the validation set, and we are not fitting anything\n",
    "        })\n",
    "        valid_acc_values.append(valid_acc)\n",
    "\n",
    "        # Print progress for the epochs\n",
    "        print('Epoch {} - valid: {:.3f} train: {:.3f} (mean)'.format(\n",
    "            epoch+1, valid_acc, np.mean(batch_acc)\n",
    "        ))   \n",
    "        \n",
    "    mean_train.append(np.mean(train_acc_values[-3:])) # save mean values of the 3 last epochs\n",
    "    mean_valid.append(np.mean(valid_acc_values[-3:]))     \n",
    "        \n",
    "    # Weights of the hidden and output layers\n",
    "    weights_hidden = W1.eval()\n",
    "    weights_output = W2.eval()\n",
    "    biases_hidden = b1.eval()\n",
    "    biases_output = b2.eval()\n",
    "    \n",
    "    # Test accuracy\n",
    "    test_acc_val = sess.run(accuracy, feed_dict={\n",
    "        X: X_test_standard, # Rescaled data\n",
    "        y: y_te,\n",
    "        W1: weights_hidden, # Set hidden weights\n",
    "        b1: biases_hidden, # Set hidden biases\n",
    "        W2: weights_output, # Set output weights\n",
    "        b2: biases_output, # Set output biases\n",
    "        dropout: False # do not apply dropout because it is the test set, and we are not fitting anything\n",
    "    })\n",
    "    print('Multilayer fully-connected network accuracy (test set): {:.3f}'.format(test_acc_val))\n",
    "    \n",
    "# Mean train and validation accuracies for the last 3 epochs might be a bit different (yet close enough) from the ones in the summary dataframe - this is due to the stochastic nature of the optimization algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a41e46e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results for training and validation for the tuned parameters\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy values\n",
    "plt.plot(train_acc_values, label='train set')\n",
    "plt.plot(valid_acc_values, label='validation set')\n",
    "plt.title('Test accuracy: {:.3f}'.format(\n",
    "    test_acc_val\n",
    "))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
